{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55160c98-98eb-4993-8a01-3d151ea19479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from simulator.model.robust_mse import RobustBidMSE\n",
    "from simulator.model.simple import SimpleBid\n",
    "from simulator.simulation.utils_visualization import plot_metric_with_error_CTR\n",
    "from simulator.validation.check_results import autobidder_check\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682594f",
   "metadata": {},
   "source": [
    "# This notebook provides a guideline how to simulate and check Robust PID Bidder for all the campaigns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "398bb46f-49cd-4aeb-9392-f57187faee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'MSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5d73a-0236-4086-b2db-939ff296c0c0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9cec7f-d89a-4412-9abe-830b82895f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "auction_mode = 'FPA'\n",
    "\n",
    "campaigns_path = \"../data/subsample_campaigns.csv\"\n",
    "stats_path = \"../data/subsample_stats.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6a2a35-6616-4f8b-9a39-12d1cbdec01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_df = pd.read_csv(campaigns_path)\n",
    "stats_df = pd.read_csv(stats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a806c09-6b92-43f0-9a73-df43d5c968ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>loc_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>campaign_start</th>\n",
       "      <th>campaign_end</th>\n",
       "      <th>auction_budget</th>\n",
       "      <th>microcat_ext</th>\n",
       "      <th>logical_category</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.695222e+07</td>\n",
       "      <td>644788.385714</td>\n",
       "      <td>3.515224e+09</td>\n",
       "      <td>5.298478e+08</td>\n",
       "      <td>5.301514e+08</td>\n",
       "      <td>1238.622143</td>\n",
       "      <td>7.844751e+05</td>\n",
       "      <td>2.769286</td>\n",
       "      <td>644358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.957660e+06</td>\n",
       "      <td>10712.608247</td>\n",
       "      <td>8.250976e+08</td>\n",
       "      <td>3.126187e+05</td>\n",
       "      <td>3.845838e+05</td>\n",
       "      <td>1906.618616</td>\n",
       "      <td>1.013575e+06</td>\n",
       "      <td>1.485689</td>\n",
       "      <td>10649.319309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.986990e+07</td>\n",
       "      <td>621630.000000</td>\n",
       "      <td>1.073987e+09</td>\n",
       "      <td>5.295283e+08</td>\n",
       "      <td>5.297184e+08</td>\n",
       "      <td>75.840000</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>621590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.307796e+07</td>\n",
       "      <td>637672.500000</td>\n",
       "      <td>2.998627e+09</td>\n",
       "      <td>5.295718e+08</td>\n",
       "      <td>5.298089e+08</td>\n",
       "      <td>228.960000</td>\n",
       "      <td>3.779750e+03</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>637650.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.567955e+07</td>\n",
       "      <td>646410.000000</td>\n",
       "      <td>3.915603e+09</td>\n",
       "      <td>5.297500e+08</td>\n",
       "      <td>5.299637e+08</td>\n",
       "      <td>547.200000</td>\n",
       "      <td>5.533500e+03</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>645790.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.077054e+07</td>\n",
       "      <td>653240.000000</td>\n",
       "      <td>4.063275e+09</td>\n",
       "      <td>5.301044e+08</td>\n",
       "      <td>5.305207e+08</td>\n",
       "      <td>1934.687500</td>\n",
       "      <td>2.096862e+06</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>653240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.776145e+07</td>\n",
       "      <td>661950.000000</td>\n",
       "      <td>4.577216e+09</td>\n",
       "      <td>5.305651e+08</td>\n",
       "      <td>5.308243e+08</td>\n",
       "      <td>13829.760000</td>\n",
       "      <td>2.278193e+06</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>661460.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        campaign_id         loc_id       item_id  campaign_start  \\\n",
       "count  7.000000e+01      70.000000  7.000000e+01    7.000000e+01   \n",
       "mean   7.695222e+07  644788.385714  3.515224e+09    5.298478e+08   \n",
       "std    4.957660e+06   10712.608247  8.250976e+08    3.126187e+05   \n",
       "min    5.986990e+07  621630.000000  1.073987e+09    5.295283e+08   \n",
       "25%    7.307796e+07  637672.500000  2.998627e+09    5.295718e+08   \n",
       "50%    7.567955e+07  646410.000000  3.915603e+09    5.297500e+08   \n",
       "75%    8.077054e+07  653240.000000  4.063275e+09    5.301044e+08   \n",
       "max    8.776145e+07  661950.000000  4.577216e+09    5.305651e+08   \n",
       "\n",
       "       campaign_end  auction_budget  microcat_ext  logical_category  \\\n",
       "count  7.000000e+01       70.000000  7.000000e+01         70.000000   \n",
       "mean   5.301514e+08     1238.622143  7.844751e+05          2.769286   \n",
       "std    3.845838e+05     1906.618616  1.013575e+06          1.485689   \n",
       "min    5.297184e+08       75.840000  2.400000e+01          1.000000   \n",
       "25%    5.298089e+08      228.960000  3.779750e+03          1.380000   \n",
       "50%    5.299637e+08      547.200000  5.533500e+03          2.350000   \n",
       "75%    5.305207e+08     1934.687500  2.096862e+06          3.410000   \n",
       "max    5.308243e+08    13829.760000  2.278193e+06          5.370000   \n",
       "\n",
       "           region_id  \n",
       "count      70.000000  \n",
       "mean   644358.000000  \n",
       "std     10649.319309  \n",
       "min    621590.000000  \n",
       "25%    637650.000000  \n",
       "50%    645790.000000  \n",
       "75%    653240.000000  \n",
       "max    661460.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campaign_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892c3e8-b025-4860-8872-8218733f16f1",
   "metadata": {},
   "source": [
    "## Robust LP Bid vs LP Bid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29feb51",
   "metadata": {},
   "outputs": [],
   "source": "# \"\"\"\n# Exact implementation of noise generation based on Wang & Dong (2023)\n# using formulas from the paper with entropy and logarithms\n\n# From the paper Wang, X., & Dong, H. (2023):\n# - Formula (15): H(y*|x*,D) = MI(y*;ω|x*,D) + E(H(y*|x*,ω))\n# - Formula (17): Ĥ(y*|x*,D) = -∑_c (1/T ∑_t p(y=c|x*,ω̂_t) log(1/T ∑_t p(y=c|x*,ω̂_t)))\n# - Formula (18): E(Ĥ(y*|x*,ω)) = -1/T ∑_{t,c} p(y=c|x*,ω̂_t) log p(y=c|x*,ω̂_t)\n# - Formula (19): M̂I(y*;ω|x*,D) = Ĥ(y*|x*,D) - E(Ĥ(y*|x*,ω))\n# \"\"\"\n\n# import numpy as np\n# import pandas as pd\n# from scipy import stats\n# import matplotlib.pyplot as plt\n\n# class WangDongCTRNoise:\n#     \"\"\"\n#     CTR noise generator based on exact formulas from Wang & Dong (2023)\n#     \"\"\"\n\n#     def __init__(self, random_state=1337):\n#         np.random.seed(random_state)\n#         self.random_state = random_state\n\n#     def monte_carlo_dropout_predictions(self, ctr_predicted, T=50, dropout_rate=0.05):\n#         \"\"\"\n#         Generate T predictions via Monte Carlo Dropout\n#         Corresponds to the procedure from the paper for obtaining ω̂_t\n\n#         Parameters:\n#         -----------\n#         ctr_predicted : array-like\n#             Base CTR predictions (p(y=1|x*,ω))\n#         T : int\n#             Number of MC samples (as in the paper)\n#         dropout_rate : float\n#             Dropout rate (0.05 recommended based on experiments in the paper)\n\n#         Returns:\n#         --------\n#         mc_predictions : np.array shape (T, N)\n#             T predictions for each of N samples\n#         \"\"\"\n#         n_samples = len(ctr_predicted)\n#         ctr_clipped = np.clip(ctr_predicted, 1e-7, 1-1e-7)  # Avoid log(0)\n\n#         mc_predictions = []\n\n#         for t in range(T):\n#             # Simulate dropout via logit modification\n#             # This is an approximation to real MC Dropout from the paper\n#             logits = np.log(ctr_clipped / (1 - ctr_clipped))\n\n#             # Dropout mask - randomly zero out some \"neurons\"\n#             dropout_mask = np.random.binomial(1, 1-dropout_rate, n_samples)\n\n#             # Add noise proportional to uncertainty\n#             # More noise where the model is less confident (mid-range CTR values)\n#             uncertainty = 4 * ctr_clipped * (1 - ctr_clipped)  # Maximum at CTR=0.5\n#             noise_std = dropout_rate * uncertainty\n\n#             logit_noise = np.random.normal(0, noise_std)\n#             noisy_logits = logits + logit_noise * dropout_mask\n\n#             # Inverse transform back to probabilities\n#             mc_ctr = 1 / (1 + np.exp(-noisy_logits))\n#             mc_predictions.append(mc_ctr)\n\n#         return np.array(mc_predictions)\n\n#     def calculate_predictive_entropy(self, mc_predictions):\n#         \"\"\"\n#         Calculate predictive entropy Ĥ(y*|x*,D)\n#         Formula (17) from the paper\n\n#         Parameters:\n#         -----------\n#         mc_predictions : np.array shape (T, N)\n#             MC predictions\n\n#         Returns:\n#         --------\n#         predictive_entropy : np.array shape (N,)\n#             Predictive entropy for each sample\n#         \"\"\"\n#         T, N = mc_predictions.shape\n\n#         # Averaged predictions: 1/T ∑_t p(y=c|x*,ω̂_t)\n#         avg_p1 = np.mean(mc_predictions, axis=0)  # P(y=1)\n#         avg_p0 = 1 - avg_p1  # P(y=0)\n\n#         # Avoid log(0)\n#         avg_p1 = np.clip(avg_p1, 1e-7, 1-1e-7)\n#         avg_p0 = np.clip(avg_p0, 1e-7, 1-1e-7)\n\n#         # Formula (17): Ĥ(y*|x*,D) = -∑_c (avg_p_c * log(avg_p_c))\n#         predictive_entropy = -(avg_p1 * np.log(avg_p1) + avg_p0 * np.log(avg_p0))\n\n#         return predictive_entropy\n\n#     def calculate_expected_entropy(self, mc_predictions):\n#         \"\"\"\n#         Calculate expected entropy E(Ĥ(y*|x*,ω))\n#         Formula (18) from the paper\n\n#         Parameters:\n#         -----------\n#         mc_predictions : np.array shape (T, N)\n#             MC predictions\n\n#         Returns:\n#         --------\n#         expected_entropy : np.array shape (N,)\n#             Expected entropy for each sample\n#         \"\"\"\n#         T, N = mc_predictions.shape\n\n#         # Avoid log(0)\n#         mc_predictions_safe = np.clip(mc_predictions, 1e-7, 1-1e-7)\n#         mc_predictions_0 = 1 - mc_predictions_safe\n\n#         # Formula (18): E(Ĥ(y*|x*,ω)) = -1/T ∑_{t,c} p(y=c|x*,ω̂_t) log p(y=c|x*,ω̂_t)\n#         entropy_per_sample = -(mc_predictions_safe * np.log(mc_predictions_safe) +\n#                               mc_predictions_0 * np.log(mc_predictions_0))\n\n#         expected_entropy = np.mean(entropy_per_sample, axis=0)\n\n#         return expected_entropy\n\n#     def calculate_mutual_information(self, predictive_entropy, expected_entropy):\n#         \"\"\"\n#         Calculate mutual information M̂I(y*;ω|x*,D)\n#         Formula (19) from the paper\n\n#         Parameters:\n#         -----------\n#         predictive_entropy : np.array\n#             Predictive entropy\n#         expected_entropy : np.array\n#             Expected entropy\n\n#         Returns:\n#         --------\n#         mutual_information : np.array\n#             Mutual information (epistemic uncertainty)\n#         \"\"\"\n#         # Formula (19): M̂I(y*;ω|x*,D) = Ĥ(y*|x*,D) - E(Ĥ(y*|x*,ω))\n#         mutual_information = predictive_entropy - expected_entropy\n\n#         # MI cannot be negative (clamp from below)\n#         mutual_information = np.maximum(mutual_information, 0)\n\n#         return mutual_information\n\n#     def generate_wang_dong_noise(self, ctr_predicted, T=50, dropout_rate=0.05,\n#                                 noise_type='total', scale_factor=1.0):\n#         \"\"\"\n#         Generate noise using the exact methodology of Wang & Dong (2023)\n\n#         Parameters:\n#         -----------\n#         ctr_predicted : array-like\n#             Predicted CTR values\n#         T : int\n#             Number of MC samples (from the paper: typically 50)\n#         dropout_rate : float\n#             Dropout rate (from the paper: typically 0.05)\n#         noise_type : str\n#             'epistemic' - epistemic uncertainty only (MI)\n#             'aleatoric' - aleatoric uncertainty only (expected entropy)\n#             'total' - total uncertainty (predictive entropy)\n#         scale_factor : float\n#             Scaling coefficient for noise\n\n#         Returns:\n#         --------\n#         noise : np.array\n#             Generated noise\n#         uncertainty_info : dict\n#             Detailed uncertainty information\n#         \"\"\"\n#         # Step 1: Generate MC predictions (dropout simulation)\n#         mc_predictions = self.monte_carlo_dropout_predictions(\n#             ctr_predicted, T=T, dropout_rate=dropout_rate\n#         )\n\n#         # Step 2: Compute three types of uncertainty using formulas from the paper\n#         predictive_entropy = self.calculate_predictive_entropy(mc_predictions)\n#         expected_entropy = self.calculate_expected_entropy(mc_predictions)\n#         mutual_information = self.calculate_mutual_information(\n#             predictive_entropy, expected_entropy\n#         )\n\n#         # Step 3: Select the uncertainty type for noise generation\n#         if noise_type == 'epistemic':\n#             # Epistemic uncertainty = MI (model uncertainty)\n#             uncertainty_values = mutual_information\n#         elif noise_type == 'aleatoric':\n#             # Aleatoric uncertainty = Expected entropy (data uncertainty)\n#             uncertainty_values = expected_entropy\n#         elif noise_type == 'total':\n#             # Total uncertainty = Predictive entropy\n#             uncertainty_values = predictive_entropy\n#         else:\n#             raise ValueError(f\"Unknown noise_type: {noise_type}\")\n\n#         # Step 4: Generate noise with variance proportional to uncertainty\n#         # The higher the uncertainty, the larger the potential noise\n#         noise_std = scale_factor * np.sqrt(uncertainty_values)\n#         noise = np.random.normal(0, noise_std)\n\n#         # Information for analysis\n#         uncertainty_info = {\n#             'predictive_entropy': predictive_entropy,\n#             'expected_entropy': expected_entropy,\n#             'mutual_information': mutual_information,\n#             'noise_std': noise_std,\n#             'mc_predictions_std': np.std(mc_predictions, axis=0),\n#             'method_params': {\n#                 'T': T,\n#                 'dropout_rate': dropout_rate,\n#                 'noise_type': noise_type,\n#                 'scale_factor': scale_factor\n#             }\n#         }\n\n#         return noise, uncertainty_info\n\n#     def suggest_scale_factor(self, ctr_predicted, target_noise_std=0.05):\n#         \"\"\"\n#         Automatically select scale_factor to achieve the desired noise level\n\n#         Parameters:\n#         -----------\n#         ctr_predicted : array-like\n#             Predicted CTR values\n#         target_noise_std : float\n#             Desired standard deviation of the noise\n\n#         Returns:\n#         --------\n#         optimal_scale_factor : float\n#             Recommended scale_factor\n#         \"\"\"\n#         # Test different scale_factor values\n#         test_scales = np.logspace(-2, 1, 20)  # from 0.01 to 10\n\n#         best_scale = 1.0\n#         min_diff = float('inf')\n\n#         for scale in test_scales:\n#             noise, _ = self.generate_wang_dong_noise(\n#                 ctr_predicted, scale_factor=scale, T=20  # Quick estimate\n#             )\n#             actual_std = np.std(noise)\n#             diff = abs(actual_std - target_noise_std)\n\n#             if diff < min_diff:\n#                 min_diff = diff\n#                 best_scale = scale\n\n#         return best_scale\n\n# # Main function for use\n# def generate_wang_dong_2023_noise(ctr_values, target_noise_std=0.05,\n#                                  noise_type='total', auto_scale=True):\n#     \"\"\"\n#     Simple function for noise generation based on Wang & Dong (2023)\n\n#     Parameters:\n#     -----------\n#     ctr_values : array-like\n#         Your predicted CTR values\n#     target_noise_std : float\n#         Desired standard deviation of the noise\n#     noise_type : str\n#         'epistemic', 'aleatoric', or 'total'\n#     auto_scale : bool\n#         Automatically select scale_factor\n\n#     Returns:\n#     --------\n#     noise : np.array\n#         Generated noise\n#     citation_info : str\n#         Ready-made text for citation in the paper\n#     \"\"\"\n#     generator = WangDongCTRNoise(random_state=42)\n\n#     if auto_scale:\n#         scale_factor = generator.suggest_scale_factor(ctr_values, target_noise_std)\n#         # print(f\"Automatically selected scale_factor: {scale_factor:.3f}\")\n#     else:\n#         scale_factor = 1.0\n\n#     noise, info = generator.generate_wang_dong_noise(\n#         ctr_values,\n#         noise_type=noise_type,\n#         scale_factor=scale_factor,\n#         T=50,  # As in the paper\n#         dropout_rate=0.05  # Optimal value from the paper\n#     )\n\n#     return noise, _ # citation_info.strip()\n\n# # Usage example\n# if __name__ == \"__main__\":\n#     # Test data - your CTR values\n#     np.random.seed(42)\n#     test_ctr = np.random.uniform(0.01, 0.3, 1000)\n\n#     print(\"=== Noise generation based on Wang & Dong (2023) ===\")\n#     noise, citation = generate_wang_dong_2023_noise(\n#         test_ctr,\n#         target_noise_std=0.05,\n#         noise_type='total'\n#     )\n\n#     print(\"\\n\" + \"=\"*50)\n#     print(\"TEXT FOR THE PAPER:\")\n#     print(\"=\"*50)\n#     print(citation)\n\n#     # Validation\n#     generator = WangDongCTRNoise()\n#     # info = generator.validate_methodology(test_ctr, plot=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461e9c7a-faa3-4485-8ec3-52f0feb61649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noised_stats_mse(stats_df, stats_save_path, old_ctr, eps, auction_mode, seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    grouped = stats_df.groupby('campaign_id')\n",
    "    for campaign_id, group in grouped:\n",
    "        old_ctr = group['CTRPredicts'].values\n",
    "\n",
    "        noise, citation = generate_wang_dong_2023_noise(\n",
    "            old_ctr,\n",
    "            target_noise_std=eps,\n",
    "            noise_type='total'\n",
    "        )\n",
    "\n",
    "        # noise = np.random.rand(old_ctr.size)\n",
    "        # noise = noise / np.linalg.norm(noise) * np.sqrt(2*eps)\n",
    "        stats_df.loc[stats_df.campaign_id == campaign_id, 'CTRPredicts_noised'] = np.clip(old_ctr + noise, 0.01, 0.1)\n",
    "\n",
    "    stats_df.to_csv(stats_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eea3c19-f756-4550-8c0f-ac3c593b39eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [15:47<00:00, 135.36s/it]\n"
     ]
    }
   ],
   "source": [
    "eps_set_data = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 1e-12]\n",
    "# eps_set_model = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 1e-12]\n",
    "\n",
    "bidder_types = ['simple', 'robust']\n",
    "seeds = list(range(10))\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "old_ctr = np.array(stats_df.CTRPredicts.copy())\n",
    "\n",
    "stats_save_path = f\"../data/data/{auction_mode.lower()}/stats_{auction_mode.lower()}_filtered_train_noised.csv\"\n",
    "\n",
    "for eps in tqdm(eps_set_data):\n",
    "    # for factor in [0.1, 1., 10, 100]:\n",
    "    # eps_data = eps_model # * factor\n",
    "    for seed in seeds:\n",
    "        # print(seed)\n",
    "        # Add noise to data\n",
    "        create_noised_stats_mse(stats_df, stats_save_path, old_ctr, eps, auction_mode, seed)\n",
    "\n",
    "        cpc = 300.\n",
    "\n",
    "        # Simple bid\n",
    "        res_simple = autobidder_check(\n",
    "            bidder=SimpleBid,\n",
    "            params={\"input_campaigns\": campaigns_path,\n",
    "                    \"input_stats\": stats_save_path,\n",
    "                    'eps': eps,\n",
    "                    'p': 1,\n",
    "                    'q': 1,\n",
    "                    'LP': True,\n",
    "                    'CPC': cpc},\n",
    "            loss_type=loss_type\n",
    "        )\n",
    "        metrics_list.append({\n",
    "            'eps': eps,\n",
    "            # 'eps_model': eps,\n",
    "            'bidder_type': 'simple',\n",
    "            'seed': seed,\n",
    "            'tvc': res_simple['score'][0],\n",
    "            'cpc_percent': res_simple['score'][1],\n",
    "            'cpc_avg': res_simple['score'][2]\n",
    "        })\n",
    "\n",
    "        # Robust bid\n",
    "        res_robust = autobidder_check(\n",
    "            bidder=RobustBidMSE,\n",
    "            params={\"input_campaigns\": campaigns_path,\n",
    "                    \"input_stats\": stats_save_path,\n",
    "                    'eps': eps,\n",
    "                    'gamma': 1.,\n",
    "                    'beta': 1.,\n",
    "                    'lambda_': 1.,\n",
    "                    'chi': 1.,\n",
    "                    'theta': 1.,\n",
    "                    'delta': 1.,\n",
    "                    'kappa': 1.,\n",
    "                    'LP': True,\n",
    "                    'CPC': cpc},\n",
    "            loss_type=loss_type\n",
    "        )\n",
    "\n",
    "        metrics_list.append({\n",
    "            'eps': eps,\n",
    "            # 'eps_model': eps_model,\n",
    "            'bidder_type': 'robust',\n",
    "            'seed': seed,\n",
    "            'tvc': res_robust['score'][0],\n",
    "            'cpc_percent': res_robust['score'][1],\n",
    "            'cpc_avg': res_robust['score'][2]\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list, columns=['eps', 'bidder_type', 'seed', 'tvc', 'cpc_percent', 'cpc_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe40dc1d-e769-40b1-aac4-f9f450682ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eps</th>\n",
       "      <th>bidder_type</th>\n",
       "      <th>mean_tvc</th>\n",
       "      <th>std_tvc</th>\n",
       "      <th>mean_cpc_percent</th>\n",
       "      <th>std_cpc_percent</th>\n",
       "      <th>mean_cpc_avg</th>\n",
       "      <th>std_cpc_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>robust</td>\n",
       "      <td>2.260368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.064523</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.260151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.108536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000e-04</td>\n",
       "      <td>robust</td>\n",
       "      <td>2.787040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>331.573948</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.000000e-04</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.260151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.108536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>robust</td>\n",
       "      <td>3.072124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>312.071788</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.260151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.108536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.000000e-03</td>\n",
       "      <td>robust</td>\n",
       "      <td>3.407976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291.231927</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.000000e-03</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.251925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>469.767519</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>robust</td>\n",
       "      <td>3.546123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>282.363859</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.172647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>474.291485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>robust</td>\n",
       "      <td>3.948277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275.151341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.000000e-02</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.074633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>528.333104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>robust</td>\n",
       "      <td>3.953850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.584078</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>simple</td>\n",
       "      <td>2.091708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.335596</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             eps bidder_type  mean_tvc  std_tvc  mean_cpc_percent  \\\n",
       "0   1.000000e-12      robust  2.260368      0.0               0.0   \n",
       "1   1.000000e-12      simple  2.260151      0.0               0.0   \n",
       "2   5.000000e-04      robust  2.787040      0.0               0.0   \n",
       "3   5.000000e-04      simple  2.260151      0.0               0.0   \n",
       "4   1.000000e-03      robust  3.072124      0.0               0.0   \n",
       "5   1.000000e-03      simple  2.260151      0.0               0.0   \n",
       "6   5.000000e-03      robust  3.407976      0.0               0.0   \n",
       "7   5.000000e-03      simple  2.251925      0.0               0.0   \n",
       "8   1.000000e-02      robust  3.546123      0.0               0.0   \n",
       "9   1.000000e-02      simple  2.172647      0.0               0.0   \n",
       "10  5.000000e-02      robust  3.948277      0.0               0.0   \n",
       "11  5.000000e-02      simple  2.074633      0.0               0.0   \n",
       "12  1.000000e-01      robust  3.953850      0.0               0.0   \n",
       "13  1.000000e-01      simple  2.091708      0.0               0.0   \n",
       "\n",
       "    std_cpc_percent  mean_cpc_avg  std_cpc_avg  \n",
       "0               0.0    468.064523          0.0  \n",
       "1               0.0    468.108536          0.0  \n",
       "2               0.0    331.573948          0.0  \n",
       "3               0.0    468.108536          0.0  \n",
       "4               0.0    312.071788          0.0  \n",
       "5               0.0    468.108536          0.0  \n",
       "6               0.0    291.231927          0.0  \n",
       "7               0.0    469.767519          0.0  \n",
       "8               0.0    282.363859          0.0  \n",
       "9               0.0    474.291485          0.0  \n",
       "10              0.0    275.151341          0.0  \n",
       "11              0.0    528.333104          0.0  \n",
       "12              0.0    280.584078          0.0  \n",
       "13              0.0    545.335596          0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_metrics = metrics_df.groupby(['eps', 'bidder_type']).agg(\n",
    "    mean_tvc=('tvc', 'mean'),\n",
    "    std_tvc=('tvc', 'std'),\n",
    "    mean_cpc_percent=('cpc_percent', 'mean'),\n",
    "    std_cpc_percent=('cpc_percent', 'std'),\n",
    "    mean_cpc_avg=('cpc_avg', 'mean'),\n",
    "    std_cpc_avg=('cpc_avg', 'std')\n",
    ").reset_index()\n",
    "\n",
    "agg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b075f016-b6a5-4649-b37c-13a28686b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metrics.to_csv(f'../results/metrics_{loss_type.lower()}_BAT_CTR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd77930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a62fa2f7",
   "metadata": {},
   "source": [
    "### TVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tvc_simple_ctr = np.array(agg_metrics[agg_metrics.bidder_type == 'simple'].mean_tvc)\n",
    "mean_tvc_robust_ctr = np.array(agg_metrics[agg_metrics.bidder_type == 'robust'].mean_tvc)\n",
    "\n",
    "# results from another notebook with normal noise\n",
    "mean_tvc_simple_norm = np.array([2.26495932, 2.26782824, 2.19687101, 2.08576958, 2.05811226,\n",
    "       1.93404816, 1.89193563])\n",
    "mean_tvc_robust_norm = np.array([2.26483722, 2.79915243, 3.03667371, 3.34679585, 3.48146273,\n",
    "       3.91136865, 4.0522365 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1035f44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00009584, 1.23312083, 1.35925602, 1.51516363, 1.63840515,\n",
       "        1.94609533, 1.93573035]),\n",
       " array([0.99994609, 1.23428767, 1.38227219, 1.60458561, 1.69158058,\n",
       "        2.02237397, 2.14184692]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_tvc_robust_ctr / mean_tvc_simple_ctr), (mean_tvc_robust_norm / mean_tvc_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "832e8f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5182667368113836, 1.5824132896383467)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_tvc_robust_ctr / mean_tvc_simple_ctr).mean(), (mean_tvc_robust_norm / mean_tvc_simple_norm).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e70b9f",
   "metadata": {},
   "source": [
    "### CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cpc_simple_ctr = np.array(agg_metrics[agg_metrics.bidder_type == 'simple'].mean_cpc_avg)\n",
    "mean_cpc_robust_ctr = np.array(agg_metrics[agg_metrics.bidder_type == 'robust'].mean_cpc_avg)\n",
    "\n",
    "# results from another notebook with normal noise\n",
    "mean_cpc_simple_norm = np.array([515.13199505, 514.81091864, 518.49015618, 523.06691103,\n",
    "       524.31452656, 525.98242848, 525.52466516])\n",
    "mean_cpc_robust_norm = np.array([515.05495171, 361.7245655 , 339.32337875, 316.72874209,\n",
    "       306.37696331, 273.53070906, 256.42070281])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c2a6a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99989679, 0.69479282, 0.65160455, 0.60698901, 0.57981732,\n",
       "        0.49342042, 0.46011147]),\n",
       " array([0.99985044, 0.70263577, 0.65444517, 0.60552242, 0.58433812,\n",
       "        0.52003773, 0.48793276]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_cpc_robust_ctr / mean_cpc_simple_ctr), (mean_cpc_robust_norm / mean_cpc_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b72eeedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6409474830510654, 0.6506803447485854)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean_cpc_robust_ctr / mean_cpc_simple_ctr).mean(), (mean_cpc_robust_norm / mean_cpc_simple_norm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978e674-efbd-456e-afd0-f0039e8b9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_with_error_CTR(\n",
    "    eps_set=eps_set,\n",
    "    agg_metrics=agg_metrics,\n",
    "    metric_mean_col='mean_tvc',\n",
    "    metric_std_col='std_tvc',\n",
    "    metric_name='TVC',\n",
    "    y_label='Total Value Clicks',\n",
    "    loss_type=loss_type\n",
    ")\n",
    "\n",
    "plot_metric_with_error_CTR(\n",
    "    eps_set=eps_set,\n",
    "    agg_metrics=agg_metrics,\n",
    "    metric_mean_col='mean_cpc_percent',\n",
    "    metric_std_col='std_cpc_percent',\n",
    "    metric_name='CPC Percent',\n",
    "    y_label='Cost per Click (%)',\n",
    "    loss_type=loss_type\n",
    ")\n",
    "\n",
    "plot_metric_with_error_CTR(\n",
    "    eps_set=eps_set,\n",
    "    agg_metrics=agg_metrics,\n",
    "    metric_mean_col='mean_cpc_avg',\n",
    "    metric_std_col='std_cpc_avg',\n",
    "    metric_name='CPC Avg',\n",
    "    y_label='Average Cost per Click',\n",
    "    loss_type=loss_type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b89e833-93ad-4515-8170-702745803f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}